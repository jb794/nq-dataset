import json
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

#(change this path to test other token types)
with open("tfidf_tokenized_questions.jsonl", "r") as f:
    dataset = [json.loads(line) for line in f]

model = SentenceTransformer("intfloat/e5-large-v2")

token_importance_all = []

for entry in dataset:
    tokens = entry.get("tfidf_tokens", [])
    original_text = " ".join(tokens)

    full_input = f"query: {original_text}"
    full_embedding = model.encode(full_input)

    token_scores = []

    for i in range(len(tokens)):
        reduced_tokens = tokens[:i] + tokens[i+1:]
        reduced_input = f"query: {' '.join(reduced_tokens)}"

        reduced_embedding = model.encode(reduced_input)
        similarity = cosine_similarity(full_embedding.reshape(1, -1), reduced_embedding.reshape(1, -1))[0][0]
        distance = 1 - similarity

        token_scores.append((tokens[i], distance))

    token_importance_all.append({
        "original_question": original_text,
        "token_scores": token_scores
    })

#(change this path to save other token types)
with open("semantic_importance_tfidf.jsonl", "w") as f:
    for item in token_importance_all:
        f.write(json.dumps(item) + "\n")
